
# 1. Introduction
> plot(child ~ parent, galton)

> plot(jitter(child,4) ~ parent,galton)

> regrline <- lm(child ~ parent, galton)

> abline(regrline, lwd=3, col='red')

> summary(regrline)

# 2. Residuals
> fit <- lm(child ~ parent, galton)

> summary(fit)
> mean(fit$residuals)

> cov(fit$residuals, galton$parent)

# sqe(ols.slope+sl,ols.intercept+ic) == deviance(fit) + sum(est(sl,ic)ˆ2 )
# sqe(ols.slope+sl,ols.intercept+ic) == sqe(ols.slope, ols.intercept) +
   sum(est(sl,ic)ˆ2 )

# extract the intercept from fit$coef
> ols.ic <- fit$coef[1]
> ols.slope <- fit$coef[2]

#Here are the vectors of variations or tweaks

sltweak <- c(.01, .02, .03, -.01, -.02, -.03)  #one for the slope

ictweak <- c(.1, .2, .3, -.1, -.2, -.3)  #one for the intercept

lhs <- numeric()

rhs <- numeric()
 
#left side of eqn is the sum of squares of residuals of the tweaked regression line

for (n in 1:6) lhs[n] <- sqe(ols.slope+sltweak[n],ols.ic+ictweak[n])

#right side of eqn is the sum of squares of original residuals + sum of squares of two tweaks

for (n in 1:6) rhs[n] <- sqe(ols.slope,ols.ic) + sum(est(sltweak[n],ictweak[n])^2)



lhs-rhs
> all.equal(lhs, rhs)


> varChild <- var(galton$child)
> varRes <- var(fit$residuals)
> varEst <- var(est(ols.slope, ols.ic))

> all.equal(varChild, varRes+varEst)

> efit <- lm(accel ~ mag+dist, attenu)
> mean(efit$residuals)

> cov(efit$residuals,  attenu$mag)

#plot the original Galton data points with larger dots for more freq pts
y <- galton$child
x <- galton$parent
freqData <- as.data.frame(table(galton$child, galton$parent))
names(freqData) <- c("child", "parent", "freq")
plot(as.numeric(as.vector(freqData$parent)), 
     as.numeric(as.vector(freqData$child)), 
     pch = 21, col = "black", bg = "lightblue",
     cex = .07 * freqData$freq, xlab = "parent", ylab = "child")

#original regression line, children as outcome, parents as predictor
abline(mean(y) - mean(x) * cor(y, x) * sd(y) / sd(x), #intercept
       sd(y) / sd(x) * cor(y, x),  #slope
       lwd = 3, col = "red")

#new regression line, parents as outcome, children as predictor
abline(mean(y) - mean(x) * sd(y) / sd(x) / cor(y, x), #intercept
       sd(y) / cor(y, x) / sd(x), #slope
       lwd = 3, col = "blue")

#assume correlation is 1 so slope is ratio of std deviations
abline(mean(y) - mean(x) * sd(y) / sd(x), #intercept
       sd(y) / sd(x),  #slope
       lwd = 2)
points(mean(x), mean(y), cex = 2, pch = 19) #big point of intersection

# 4. Residual Variation

> fit <- lm(child ~ parent, galton)

> sqrt(sum(fit$residuals^2)/(n-2))
> summary(fit)$sigma # ==
> sqrt(deviance(fit)/(n-2))

> mu <- mean(galton$child)
> sTot <- sum((galton$child-mu)^2)

> sRes <- deviance(fit)
> 1.-sRes/sTot ==> R^2
> summary(fit)$r.squared #  ==

> cor(galton$child, galton$parent)^2


# 5. Multivariable Regression

> ones <- rep(1, nrow(galton))

# dont use the default interception, 
# instead use the variable ones we created
> lm(child ~ ones + parent -1, galton)
> lm(child ~ parent, galton) # ==

# Thus, subtracting
| the mean is equivalent to replacing a variable by the residual of its regression against 1.

> lm(child ~ 1, galton) # == mean(galton$child)

# The general technique is to pick one predictor and to replace all other variables by the residuals of
| their regressions against that one. 

# fit togather
> fit <- lm(Volume ~ Girth + Height + Constant -1, trees)

# fit one by one
> trees2 <- eliminate("Girth", trees)
> head(trees2)

> fit2 <- lm(Volume ~ Height + Constant -1, trees2)

> lapply(list(fit, fit2), coef)

# Regress the given variable on the given predictor,
# suppressing the intercept, and return the residual.
regressOneOnOne <- function(predictor, other, dataframe){
  # Point A. Create a formula such as Girth ~ Height -1
  formula <- paste0(other, " ~ ", predictor, " - 1")
  # Use the formula in a regression and return the residual.
  resid(lm(formula, dataframe))
}

# Eliminate the specified predictor from the dataframe by
# regressing all other variables on that predictor
# and returning a data frame containing the residuals
# of those regressions.
eliminate <- function(predictor, dataframe){
  # Find the names of all columns except the predictor.
  others <- setdiff(names(dataframe), predictor)
  # Calculate the residuals of each when regressed against the given predictor
  temp <- sapply(others, function(other)regressOneOnOne(predictor, other, dataframe))
  # sapply returns a matrix of residuals; convert to a data frame and return.
  as.data.frame(temp)
}




# 6. MultiVar Examples


makelms <- function(){
  # Store the coefficient of linear models with different independent variables
  cf <- c(coef(lm(Fertility ~ Agriculture, swiss))[2], 
          coef(lm(Fertility ~ Agriculture + Catholic,swiss))[2],
          coef(lm(Fertility ~ Agriculture + Catholic + Education,swiss))[2],
          coef(lm(Fertility ~ Agriculture + Catholic + Education + Examination,swiss))[2],
          coef(lm(Fertility ~ Agriculture + Catholic + Education + Examination +Infant.Mortality, swiss))[2])
  print(cf)
}

# Regressor generation process 1.
rgp1 <- function(){
  print("Processing. Please wait.")
  # number of samples per simulation
  n <- 100
  # number of simulations
  nosim <- 1000
  # set seed for reproducability
  set.seed(4321)
  # Point A:
  x1 <- rnorm(n)
  x2 <- rnorm(n)
  x3 <- rnorm(n)
  # Point B:
  betas <- sapply(1 : nosim, function(i)makelms(x1, x2, x3))
  round(apply(betas, 1, var), 5)
}

# Regressor generation process 2.
rgp2 <- function(){
  print("Processing. Please wait.")
  # number of samples per simulation
  n <- 100
  # number of simulations
  nosim <- 1000
  # set seed for reproducability
  set.seed(4321)
  # Point C:
  x1 <- rnorm(n)
  x2 <- x1/sqrt(2) + rnorm(n) /sqrt(2)
  x3 <- x1 * 0.95 + rnorm(n) * sqrt(1 - 0.95^2)
  # Point D:
  betas <- sapply(1 : nosim, function(i)makelms(x1, x2, x3))
  round(apply(betas, 1, var), 5)
}

# swiss data

> all <- lm( Fertility ~ ., data=swiss)
> summary(all)

> summary(lm(Fertility ~ Agriculture, data=swiss))

> cor(swiss$Examination, swiss$Education)
> cor(swiss$Agriculture, swiss$Education)

> makelms() 

> ec <- swiss$Examination+swiss$Catholic

> efit <- lm(Fertility ~ . + ec, data=swiss)

> all$coefficients-efit$coefficients



# 7. MultiVar Examples2
# R function sapply to find out the classes of the columns of the data.
> sapply(InsectSprays, class)

> fit <- lm(count ~ spray, data=InsectSprays)
> fit$coefficients
> summary(fit)$coef
> est <- summary(fit)$coef[,1]

> nfit <- lm(count ~ spray-1, data=InsectSprays)
> summary(nfit)$coef

> spray2 <- relevel(InsectSprays$spray, "C")

> fit2 <- lm(InsectSprays$count ~ spray2)

# 8: MultiVar Examples3
> dim(hunger)
> names(hunger)

> fit <- lm(Numeric ~ Year, hunger)

> summary(fit)$coef

lmF <- lm(hunger$Numeric[hunger$Sex=="Female"] ~ hunger$Year[hunger$Sex=="Female"]) 
 lmF <- lm(Numeric[Sex=="Female"] ~| Year[Sex=="Female"],hunger)

> lmM <- lm(hunger$Numeric[hunger$Sex=="Male"] ~ hunger$Year[hunger$Sex=="Male"])

> lmBoth <- lm(Numeric ~ Year+Sex, hunger)

> summary(lmBoth)



> lmInter <- lm(Numeric ~ Year+Sex+Sex*Year, data=hunger)
> summary(lmInter)



# 9: Residuals Diagnostics and Variation 

> fit <- lm(y ~ x, out2)
> plot(fit, which=1)
> fitno <- lm(y ~ x, out2[-1,])
> plot(fitno, which=1)

# see the change induced by including the influential first sample.
> coef(fit)-coef(fitno)

#  dfbeta, does the equivalent calculation for every sample in the data.
>  head(dfbeta(fit)) 

# calcluate  influence, sometimes leverage, and sometimes hat value. 
> resno <- out2[1, "y"] - predict(fitno, out2[1,])

> 1-resid(fit)[1]/resno
>  head(hatvalues(fit))

> sigma <- sqrt(sum(fit$residuals^2)/fit$df.residual)
> rstd <- resid(fit)/(sigma * sqrt(1-hatvalues(fit)))
# === rstandard(fit)
> head(cbind(rstd, rstandard(fit)))

#  shows the square root of standardized residuals against fitted values
> plot(fit,which=3)

# a QQ plot of standardized residuals against normal with constant variance.
> plot(fit, which=2)


# Studentized residuals,
> sigma1 <- sqrt(sum(fitno$residuals^2)/fitno$df.residual)
> resid(fit)[1]/(sigma1*sqrt(1-hatvalues(fit)[1]))

# rstudent, calculates Studentized residuals for each sample 
>  head(rstudent(fit))

# Cook's distance

> dy <- predict(fitno, out2)-predict(fit, out2)
>  sum(dy^2)/(2*sigma^2) # 2 is the number of predictor
# ==
> cooks.distance(fit)[1] 

> plot(fit, which=5)

